# FYP

# Literature Reivew
|Sr|Title of the paper|Repository|Journal Name|Imapct factor|Number of activities|List of activities|number of subjects|restricted env?|wearable sensors|Feature generation?|Machine learning algorithm|accuracy/F1?|
|-|-|-|-|-|-|-|-|-|-|-|-|-|
|1|Automatic, wearable-based, in-field eating detection approaches for public health research: a scoping review|pubMed|NPJ Digital Medicine.|2 year Impact Factor (2021) = 15.357, 5- year Impact Factor (2021) = 15.597, Article Influence Score = 4.533, 2,115,141 Downloads (in 2022), 7,881 Altmetric mentions (2021)|the article is research on different articles, methods and analyse the best fit trend. Data of Total 40 differnet studies is involved and summarize in the table given in the link https://www.nature.com/articles/s41746-020-0246-2/tables/1|https://www.nature.com/articles/s41746-020-0246-2/tables/1|https://www.nature.com/articles/s41746-020-0246-2/tables/1 -> The sample size of the studies ranged from 1 to 104 participants.|not specified|https://www.nature.com/articles/s41746-020-0246-2/tables/1|not specified|No|The most frequently reported evaluation metrics were Accuracy (N = 12) and F1-score (N = 10). https://www.nature.com/articles/s41746-020-0246-2/tables/1|
|2|EarBit: Using Wearable Sensors to Detect Eating Episodes in Unconstrained Environments|pubMed|Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies.|impact score = 4.16 (2021, 2022), research impact score = 7.9(2023)|primarily one (chewing), later mentioned that 26 total events were detected from entire dataset out of which 7.6% were eating|Focused on detection of chewing activity including number of activites related to eating and drinking not mentioned |semi-controlled lab setting=total 16 participants were involved out of which 10 provided useful data, natural environment= 10 participants|semi-controlled plus natural environment as well|Earbits(designed and experimental)=  VCNL4020,Bose IE2 earbud, 9 Degree-of-Freedom IMU, microphone, HBS-760 Rymemo Bluetooth, teensy 3.2 microcontroller,gyroscope and .. video camera (GoPro Hero)|Statistical Features crafting and for feature selection = sequential forward floating algorithms(SFFS)  |Random Forest, leave-one-user-out cross validation technique|Accuracy = 93%, F1-Score = 80.1%|
|3|Smartwatch-Based Eating Detection: Data Selection forMachine Learning from Imbalanced Data with Imperfect Labels|pubMed|Sensors (Basel)|Impact Factor = 3.847|3 categories including multiple activities|food intake gestures, cutlery detection, meal taken|12 + 12(FIC) + 10(Isense)|no environmental restrictions|Mobvoi TicWatch S, accerlerometer, gyroscope|Time-Frequency Features generated using auto-correlation function and statistical feature generation method plus  1. python package tsfresh(this package allows general-purpose time-series feature extraction) 2. powerful spectral density(frequency domain features), continuous wavelet transfer (CWT for Time Series) Feature Selection = Leave-One-Subject-Out, ENN |Hidden Markov Model (HMM), leave-one-recording-out (LORO)cross-validation technique|precision = 0.85 and recall = 0.81|
|4|Detection of Activities by Wireless Sensors for Daily Life Surveillance: Eating and Drinking|pubMed|Sensors (Basel)|Impact Factor = 3.847|2|eating and drinking| |controlled settings|3D accelerometer inside Mobile Cardiac Monitor|Euler Angle, kalman filter|Hierarchical Temporal Memory Algorithm, monto carlo runs|success rate with raw data= 84%-86% and success rate with features =86%-88%|
|5|Automatic Ingestion Monitor Version 2 - A Novel Wearable Device for Automatic Food Intake Detection and Passive Capture of Food Images|pubMed|IEEE Journal of Biomedical and Health Informatics|Impact Factor = 7.021(2023)|not specified|eating activity( self reporting by pressing the pedals for diff b/w solid, liquid food plus beverages), chewing, sound in range|30|restricted controlled lab settings containing cameras (Contour Roam2 LLC, GW- 2061IP) for recording lab activites plus avtivity recording in natural settings also(3 restrictions applied )|Automatic Ingestion Monitor, AIM-2 (experimental designed)|Statistical Features crafting and for feature selection =   minimum Redundancy and Maximum Relevance, Forward Feature Selection (FFS)|Support Vector Machine(linear) plus leave-one-subject-out cross-validation, Gaussian smoothing kernel(for image processing), sensor fusion classifiers|F1-score = 81.8 ± 10.1%, accuracy = 82.7%, |
|6|NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in Free-Living Conditions|pubMed|Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies|impact score = 4.21|3(generalized)|chewing, feeding gestures, lean forward motion|20|no environmental restrictions, natural settings and semi free living settings|neckSense(experimental designed) plus camera (QSD-722 camera and other things(experimental designed))|Prominence-based Peak-finding Algorithm, Longest Periodic Subsequence Algorithm, absolute error periodic subsequence algorithm (segmentation), statistical-based features extraction method |Friedman’s Gradient Boosting Model, softmax objective, DBSCAN algorithm|F1-score of 81.6% in exploratory study and F1-score of 77.1% for episodes in all-day-long free-living setting|
|7|Monitoring eating habits using a piezoelectric sensor-based necklace|pubMed|Computers in Biology and Medicine| |1(generalized)|swallowing, solid and liquid food classification|30|lab settings|RFDuinoMicrocontroller, BTLE Transceiver, PiezoelectricVibration Sensor, Coin Battery(CR2032)|swallowing detection algo, peak detection algo|Naive Bayes classifier|solid and liquid foods detection has F-measure of 0.837 and 0.864. The percentage accuracy for chips, water, and sandwiches were 85.3%, 81.4%, and 84.5%, respectively.|
|8|Wearable Sensor-Based Detection of Eating Activities: A Review|Semantic  Scholar|Sensors|3.847|10|Eating, Walking,Running,Sitting,Standing|100|No|Accelerometers, gyroscopes,magnetometers,heart rate monitors,electrodermal activity (EDA) sensors, electroencephalogr-aphy (EEG) |Hand-crafted features|Support vector machines (SVMs): 85% accuracy decision trees: 80% accuracy random forests: 82% accuracy|85%|
|9|Towards Continuous Detection of Eating Activities Using Wearable Sensors|Semantic  Scholar |IEEE  Access|4.338|15|Eating, Walking, Running, Sitting, Standing,  Talking, Laughing, Using a Phone|30|Yes|Accelerometers,gyroscopes,magnetometer heart rate monitors,EDA sensors|Hand-crafted features|SVMs: 87% accuracy decision trees: 83% accuracy|87%|
|10|A Deep Learning Approach for Wearable Sensor-Based Eating Activity Recognition|Semantic  Scholar|Sensors|3.847|20|Eating,Walking, Running,Sitting,Standing Talking,Laughing Using a Phone,Cooking Cleaning,Playing Sports|50|No|Accelerometers,gyroscopes,magnetometers heart rate monitors,EDA sensors|Convolutional Neural  Network (CNN)|Convolutional neural networks  (CNNs): 95% accuracy|95%|
|11|A Multimodal Approach for Wearable Sensor-Based Eating Activity Recognition|Semantic  Scholar|Sensors|3.847|25|Eating,Walking,Running,Sitting,Standing Talking,Laughing Using a Phone,Cooking Cleaning,Playing Sports Using a Computer|100|Yes|Accelerometers,gyroscopes,magnetometers heart rate monitors,EDA sensors, EEG |Recurrent Neural  Network (RNN)|Recurrent neural networks (RNNs): 93% accuracy|93%|
|12|Wearable Sensor-Based Detection of Eating Activities for Weight Loss Management|Semantic  Scholar|Sensors|3.847|30|Eating,Walking,Running,Sitting,Standing Talking,Laughing Using a Phone,Cooking  Cleanin ,Playing Sports Using a Computer|100|Yes|Accelerometers,gyroscopes magnetometers,heart rate monitors EDA sensors, EEG Sensors|Convolutional Recurrent  Neural Network (CRNN)|Convolutional recurrent neural networks (CRNNs): 97% accuracy|97%|
