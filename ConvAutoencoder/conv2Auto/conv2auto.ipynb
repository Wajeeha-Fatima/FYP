{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 00:31:29.360012: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-01 00:31:29.735637: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-01 00:31:29.738456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 00:31:31.097387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CNN.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1RDQrJejOfArNgzjcXMDQ0C4bBzMljDtA\n",
    "\n",
    "# Connect with Google Drive\n",
    "\"\"\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\"\"\"# Importing Libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\"\"\"# Loading Data\"\"\"\n",
    "\n",
    "path = \"../../Cog_DataSets/testing/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/testing/\"\n",
    "test_acc = np.load(path+\"testAccelerometer.npy\")\n",
    "test_grav = np.load(path+\"testGravity.npy\")\n",
    "test_gyro = np.load(path+\"testGyroscope.npy\")\n",
    "test_jinsAcc = np.load(path+\"testJinsAccelerometer.npy\")\n",
    "test_jinsGyro = np.load(path+\"testJinsGyroscope.npy\")\n",
    "test_Label =np.load(path+\"testLabels.npy\") \n",
    "test_linAcc = np.load(path+\"testLinearAcceleration.npy\")\n",
    "test_MsAcc = np.load(path+\"testMSAccelerometer.npy\")\n",
    "test_MsGyro = np.load(path + \"testMSGyroscope.npy\")\n",
    "test_MsMag = np.load(path+\"testMagnetometer.npy\")\n",
    "# test_acc\n",
    "\n",
    "path = \"../../Cog_DataSets/training/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/training/\"\n",
    "train_acc = np.load(path+\"trainAccelerometer.npy\")\n",
    "train_grav = np.load(path+\"trainGravity.npy\")\n",
    "train_gyro = np.load(path+\"trainGyroscope.npy\")\n",
    "train_jinsAcc = np.load(path+\"trainJinsAccelerometer.npy\")\n",
    "train_jinsGyro = np.load(path+\"trainJinsGyroscope.npy\")\n",
    "train_Label =np.load(path+\"trainLabels.npy\") \n",
    "train_linAcc = np.load(path+\"trainLinearAcceleration.npy\")\n",
    "train_MsAcc = np.load(path+\"trainMSAccelerometer.npy\")\n",
    "train_MsGyro = np.load(path + \"trainMSGyroscope.npy\")\n",
    "train_MsMag = np.load(path+\"trainMagnetometer.npy\")\n",
    "# print(train_Label.shape)\n",
    "\n",
    "\n",
    "def Normalize(X):\n",
    "  norm = []\n",
    "  for I in range(len(X)):\n",
    "    norm.append(normalize(X[I]))\n",
    "  norm=np.array(norm)\n",
    "  return norm\n",
    "\n",
    "train_acc = Normalize(train_acc)\n",
    "train_gyro = Normalize(train_gyro)\n",
    "train_grav = Normalize(train_grav)\n",
    "train_linAcc = Normalize(train_linAcc)\n",
    "train_MsMag = Normalize(train_MsMag)\n",
    "train_MsAcc = Normalize(train_MsAcc)\n",
    "train_MsGyro = Normalize(train_MsGyro)\n",
    "train_jinsAcc = Normalize(train_jinsAcc)\n",
    "train_jinsGyro = Normalize(train_jinsGyro)\n",
    "\n",
    "test_acc = Normalize(test_acc)\n",
    "test_gyro = Normalize(test_gyro)\n",
    "test_grav = Normalize(test_grav)\n",
    "test_linAcc = Normalize(test_linAcc)\n",
    "test_MsMag = Normalize(test_MsMag)\n",
    "test_MsAcc = Normalize(test_MsAcc)\n",
    "test_MsGyro = Normalize(test_MsGyro)\n",
    "test_jinsAcc = Normalize(test_jinsAcc)\n",
    "test_jinsGyro = Normalize(test_jinsGyro)\n",
    "\n",
    "\n",
    "\n",
    "# # all data of shape #,400,3\n",
    "# # adding all relative data.\n",
    "# # Mobile training accelerometer + Mobile testing accelerometer data\n",
    "# train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "# train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "# train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "# train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "# train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "# train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "# train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "# train_jinsAcc_reshaped = np.append(train_jinsAcc_reshaped,test_jinsAcc_reshaped, axis=0)\n",
    "# train_jinsGyro_reshaped = np.append(train_jinsGyro_reshaped,test_jinsGyro_reshaped, axis=0)\n",
    "\n",
    "\n",
    "# print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "#                        , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "#                        train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# # all data of shape 4572,400,3\n",
    "# # 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# # stack\n",
    "# all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "#                        , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "#                        train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "# all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSample(data):\n",
    "    new_length = 80\n",
    "    old_length = data.shape[1]\n",
    "\n",
    "    x_downsampled = np.zeros((data.shape[0], new_length, 3))\n",
    "\n",
    "    for i in range(2284):\n",
    "        for j in range(3):\n",
    "            f = interp1d(np.arange(old_length), data[i, :, j], kind='linear')\n",
    "            x_downsampled[i, :, j] = f(np.linspace(0, old_length - 1, new_length))\n",
    "    \n",
    "    return x_downsampled\n",
    "            \n",
    "train_acc_reshaped = downSample(train_acc)\n",
    "train_gyro_reshaped = downSample(train_gyro)\n",
    "train_grav_reshaped = downSample(train_grav)\n",
    "train_linAcc_reshaped = downSample(train_linAcc)\n",
    "train_MsAcc_reshaped = downSample(train_MsAcc)\n",
    "train_MsGyro_reshaped = downSample(train_MsGyro)\n",
    "train_MsMag_reshaped = downSample(train_MsMag)\n",
    "\n",
    "test_acc_reshaped = downSample(test_acc)\n",
    "test_gyro_reshaped = downSample(test_gyro)\n",
    "test_grav_reshaped = downSample(test_grav)\n",
    "test_linAcc_reshaped = downSample(test_linAcc)\n",
    "test_MsAcc_reshaped = downSample(test_MsAcc)\n",
    "test_MsGyro_reshaped = downSample(test_MsGyro)\n",
    "test_MsMag_reshaped = downSample(test_MsMag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all sensors after up/down sample...  (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "# all data of shape #,80,3\n",
    "# adding all relative data.\n",
    "# Mobile training accelerometer + Mobile testing accelerometer data\n",
    "train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "train_jinsAcc_reshaped = np.append(train_jinsAcc,test_jinsAcc, axis=0)\n",
    "train_jinsGyro_reshaped = np.append(train_jinsGyro,test_jinsGyro, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "                       , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "                       train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# all data of shape 4572,400,3\n",
    "# 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# stack\n",
    "all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "                       , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "                       train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training + validation Data\n",
      "(3657, 80, 3, 9) (915, 80, 3, 9)\n",
      "Shape of training + validation Labels\n",
      "(3657,) (915,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 90% training data + labels\n",
    "# train_data = all_data[: int(all_data.shape[0]*0.9)]\n",
    "# # 10% testing data + labels\n",
    "# test_data = all_data[int(all_data.shape[0]*0.9):]\n",
    "# train_labels = all_Label[: int(all_Label.shape[0]*0.9)]\n",
    "# test_labels = all_Label[int(all_Label.shape[0]*0.9):]\n",
    "\n",
    "# print(\"\\nShape of training and testin data + labels...\\n\")\n",
    "# print(train_data.shape, test_data.shape)\n",
    "# print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(all_data, all_Label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training + validation Data\")\n",
    "print(np.shape(x_train), np.shape(x_val))\n",
    "\n",
    "# y_train = to_categorical(y_train, num_classes=55)\n",
    "# y_val = to_categorical(y_val, num_classes=55)\n",
    "\n",
    "print(\"Shape of training + validation Labels\")\n",
    "print(np.shape(y_train), np.shape(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: ------------------------------\n",
      "(80, 3, 9)\n",
      "Batch norm shape: ------------------------------\n",
      "(None, 80, 3, 9)\n",
      "Conv1 shape: ------------------------------\n",
      "(None, 80, 3, 32)\n",
      "Pool1 shape: ------------------------------\n",
      "(None, 40, 2, 32)\n",
      "Conv2 shape: ------------------------------\n",
      "(None, 40, 2, 64)\n",
      "Pool2 shape: ------------------------------\n",
      "(None, 20, 1, 64)\n",
      "Dense layer size: ---------------------------\n",
      "(None, 11520)\n",
      "Reshape param: -----------------------------\n",
      "11520\n",
      "Reshape shape: ------------------------------\n",
      "(None, 20, 9, 64)\n",
      "Upsampling2 shape: ------------------------------\n",
      "(None, 40, 18, 64)\n",
      "Deconv2 shape: ------------------------------\n",
      "(None, 40, 18, 64)\n",
      "Upsampling1 shape: ------------------------------\n",
      "(None, 80, 36, 64)\n",
      "Deconv1 shape: ------------------------------\n",
      "(None, 80, 36, 32)\n",
      "Batch norm shape: ------------------------------\n",
      "(None, 80, 36, 32)\n",
      "Output shape: ------------------------------\n",
      "(None, 80, 36, 1)\n"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMTERS\n",
    "\n",
    "# Filter parameters, i.e. about the number of inputs processed by each neuron of the convolutional layer\n",
    "filter1Size = (11,1)\n",
    "filter2Size = (13,1)\n",
    "filter3Size = (13,1)\n",
    "\n",
    "# Downsampling factors of the pooling layers\n",
    "poolingLayer1Factor = (2,1)\n",
    "poolingLayer2Factor = (3,1)\n",
    "poolingLayer3Factor = (2,1)\n",
    "\n",
    "# Number of feature maps processed by each convolutional layer\n",
    "nbFeaturesLayer1 = 50\n",
    "nbFeaturesLayer2 = 40\n",
    "nbFeaturesLayer3 = 30\n",
    "\n",
    "# Activation function of the convolutional layer(s)\n",
    "activationConv = 'relu'\n",
    "\n",
    "# Output dimension of the LSTM\n",
    "outputLSTM = 55\n",
    "\n",
    "# Parameters of the dense layer\n",
    "activationMLP = 'relu'\n",
    "inputMLP = 100\n",
    "\n",
    "# Training parameters\n",
    "batchSize =32\n",
    "numberOfEpochs = 10\n",
    "learningRate = 0.001\n",
    "\n",
    "input_shape = (80,3,9)\n",
    "nbClasses = 55\n",
    "timeWindow = 80\n",
    "nbSensors = 9\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# convAutoencoder: define a convolutional autoencoder model with 2 conv+pooling layers\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# def conv2Autoencoder(\n",
    "#     inputShape,\n",
    "#     nkerns,\n",
    "#     filterSizes,\n",
    "#     poolSizes,\n",
    "#     activationConv,\n",
    "#     timeWindow,\n",
    "#     nbSensors,\n",
    "#     activationMLP,\n",
    "#     decoder=True):\n",
    "\n",
    "#     #outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)/poolSizes[0][0]\n",
    "#     #inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "#     # NOTE: if padding = 'same', the size of the feature maps doesn't change after being convoluted\n",
    "#     outputSizeLastConv = timeWindow/poolSizes[0][0]/poolSizes[1][0]\n",
    "#     inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     print('Input shape: ------------------------------')\n",
    "#     print(inputShape)\n",
    "\n",
    "#     # Batch normalization layer\n",
    "#     model.add(BatchNormalization(input_shape=inputShape))\n",
    "#     print('Batch norm shape: ------------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # First convolutional layer\n",
    "#     # Note: default padding = zero padding?\n",
    "#     model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "#     print('Conv1 shape: ------------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # Max-pooling layer\n",
    "#     model.add(MaxPooling2D(pool_size=poolSizes[0], padding='same'))\n",
    "#     print('Pool1 shape: ------------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # Second convolutional layer\n",
    "#     # Note: default padding = zero padding?\n",
    "#     model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "#     print('Conv2 shape: ------------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # Max-pooling layer\n",
    "#     model.add(MaxPooling2D(pool_size=poolSizes[1], padding='same'))\n",
    "#     print('Pool2 shape: ------------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # Dense layer\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(inputMLP, activation=activationMLP))\n",
    "#     print('Dense layer size: ---------------------------')\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "\n",
    "#     if decoder:\n",
    "\n",
    "#         # Reshaping layer\n",
    "#         print('Reshape param: -----------------------------')\n",
    "#         print(outputSizeLastConv*nbSensors*nkerns[1])\n",
    "#         model.add(Reshape((int(outputSizeLastConv),int(nbSensors),int(nkerns[1]))))\n",
    "#         print('Reshape shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # First up-sampling layer\n",
    "#         model.add(UpSampling2D(size=poolSizes[1]))\n",
    "#         print('Upsampling2 shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # First deconvolutional layer\n",
    "#         model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "#         print('Deconv2 shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # Second up-sampling layer\n",
    "#         model.add(UpSampling2D(size=poolSizes[0]))\n",
    "#         print('Upsampling1 shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # Second deconvolutional layer\n",
    "#         model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "#         print('Deconv1 shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # Batch normalization layer\n",
    "#         model.add(BatchNormalization())\n",
    "#         print('Batch norm shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         # Output layer\n",
    "#         model.add(Conv2D(1, kernel_size=filterSizes[0], activation='linear', padding='same')) \n",
    "#         print('Output shape: ------------------------------')\n",
    "#         print(model.layers[-1].output_shape)      \n",
    "\n",
    "#     return model\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# convAutoencoder: define a convolutional autoencoder model with 2 conv+pooling layers\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def conv2Autoencoder(\n",
    "    inputShape,\n",
    "    nkerns,\n",
    "    filterSizes,\n",
    "    poolSizes,\n",
    "    activationConv,\n",
    "    timeWindow,\n",
    "    nbSensors,\n",
    "    activationMLP,\n",
    "    decoder=True):\n",
    "\n",
    "    # NOTE: if padding = 'same', the size of the feature maps doesn't change after being convoluted\n",
    "    outputSizeLastConv = int(np.ceil(timeWindow/poolSizes[0][0]/poolSizes[1][0]))\n",
    "    inputMLP = nkerns[1]*outputSizeLastConv*nbSensors\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    print('Input shape: ------------------------------')\n",
    "    print(inputShape)\n",
    "\n",
    "    # Batch normalization layer\n",
    "    model.add(BatchNormalization(input_shape=inputShape))\n",
    "    print('Batch norm shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # First convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "    print('Conv1 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[0], padding='same'))\n",
    "    print('Pool1 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "    print('Conv2 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[1], padding='same'))\n",
    "    print('Pool2 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(inputMLP, activation=activationMLP))\n",
    "    print('Dense layer size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "\n",
    "    if decoder:\n",
    "\n",
    "        # Reshaping layer\n",
    "        print('Reshape param: -----------------------------')\n",
    "        print(outputSizeLastConv*nbSensors*nkerns[1])\n",
    "        model.add(Reshape((outputSizeLastConv, nbSensors, nkerns[1])))\n",
    "        print('Reshape shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # First up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[1]))\n",
    "        print('Upsampling2 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # First deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "        print('Deconv2 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[0]))\n",
    "        print('Upsampling1 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "        print('Deconv1 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Batch normalization layer\n",
    "        model.add(BatchNormalization())\n",
    "        print('Batch norm shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Conv2D(1, kernel_size=filterSizes[0], activation='linear', padding='same')) \n",
    "        print('Output shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)      \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (80, 3, 9)\n",
    "nkerns = [32, 64]\n",
    "filterSizes = [(3, 3), (3, 3)]\n",
    "poolSizes = [(2, 2), (2, 2)]\n",
    "activationConv = 'relu'\n",
    "timeWindow = input_shape[0]\n",
    "nbSensors = input_shape[2]\n",
    "activationMLP = 'relu'\n",
    "\n",
    "model = conv2Autoencoder(\n",
    "    inputShape=input_shape,\n",
    "    nkerns=nkerns,\n",
    "    filterSizes=filterSizes,\n",
    "    poolSizes=poolSizes,\n",
    "    activationConv=activationConv,\n",
    "    timeWindow=timeWindow,\n",
    "    nbSensors=nbSensors,\n",
    "    activationMLP=activationMLP\n",
    "    )\n",
    "\n",
    "# model = conv2Autoencoder(inputShape=input_shape,\n",
    "#                             nkerns=[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n",
    "#                             filterSizes=[filter1Size, filter2Size, filter3Size],\n",
    "#                             poolSizes=[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n",
    "#                             activationConv=activationConv,\n",
    "#                             timeWindow=timeWindow,\n",
    "#                             nbSensors=nbSensors,\n",
    "#                             activationMLP='sigmoid'\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "115/115 [==============================] - 73s 614ms/step - loss: 800.5683 - accuracy: 0.0203 - val_loss: 443.3828 - val_accuracy: 0.0207\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 72s 626ms/step - loss: 393.4245 - accuracy: 0.0205 - val_loss: 734.0956 - val_accuracy: 0.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94f860efe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=Adam(learning_rate=learningRate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# model.fit(x=x_train, y=x_train, epochs=2, batch_size=batchSize, validation_data=(y_train, y_train))\n",
    "model.fit(all_data, all_Label, epochs=2, batch_size=batchSize, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 15s 128ms/step\n",
      "29/29 [==============================] - 4s 139ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_train_comp = model.predict(x_train)\n",
    "prediction_test_comp = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # x_train_compressed = model.predict(train_data)\n",
    "# # x_test_compressed = model.predict(test_data)\n",
    "\n",
    "# predict_train_comp = np.reshape(predict_train_comp, (predict_train_comp.shape[0], 80*9))\n",
    "# # Train an SVM classifier on the compressed data\n",
    "# svm = SVC(kernel='linear', C=1, gamma='auto')\n",
    "# svm.fit(predict_train_comp, y_train)\n",
    "\n",
    "# # print(x_train_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the accuracy of the SVM classifier\n",
    "# prediction_test_comp = np.reshape(prediction_test_comp, (prediction_test_comp.shape[0], 80*9))\n",
    "# svm_score = svm.score(prediction_test_comp, y_val)\n",
    "# print(\"Accuracy: %.2f%%\" % (svm_score*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
