{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 15:00:25.219332: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 15:00:25.312202: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 15:00:25.313717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 15:00:26.351579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\"\"\"# Loading Data\"\"\"\n",
    "\n",
    "path = \"../../Cog_DataSets/testing/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/testing/\"\n",
    "test_acc = np.load(path+\"testAccelerometer.npy\")\n",
    "test_grav = np.load(path+\"testGravity.npy\")\n",
    "test_gyro = np.load(path+\"testGyroscope.npy\")\n",
    "test_jinsAcc = np.load(path+\"testJinsAccelerometer.npy\")\n",
    "test_jinsGyro = np.load(path+\"testJinsGyroscope.npy\")\n",
    "test_Label =np.load(path+\"testLabels.npy\") \n",
    "test_linAcc = np.load(path+\"testLinearAcceleration.npy\")\n",
    "test_MsAcc = np.load(path+\"testMSAccelerometer.npy\")\n",
    "test_MsGyro = np.load(path + \"testMSGyroscope.npy\")\n",
    "test_MsMag = np.load(path+\"testMagnetometer.npy\")\n",
    "# test_acc\n",
    "\n",
    "path = \"../../Cog_DataSets/training/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/training/\"\n",
    "train_acc = np.load(path+\"trainAccelerometer.npy\")\n",
    "train_grav = np.load(path+\"trainGravity.npy\")\n",
    "train_gyro = np.load(path+\"trainGyroscope.npy\")\n",
    "train_jinsAcc = np.load(path+\"trainJinsAccelerometer.npy\")\n",
    "train_jinsGyro = np.load(path+\"trainJinsGyroscope.npy\")\n",
    "train_Label =np.load(path+\"trainLabels.npy\") \n",
    "train_linAcc = np.load(path+\"trainLinearAcceleration.npy\")\n",
    "train_MsAcc = np.load(path+\"trainMSAccelerometer.npy\")\n",
    "train_MsGyro = np.load(path + \"trainMSGyroscope.npy\")\n",
    "train_MsMag = np.load(path+\"trainMagnetometer.npy\")\n",
    "# print(train_Label.shape)\n",
    "\n",
    "\n",
    "def Normalize(X):\n",
    "  norm = []\n",
    "  for I in range(len(X)):\n",
    "    norm.append(normalize(X[I]))\n",
    "  norm=np.array(norm)\n",
    "  return norm\n",
    "\n",
    "train_acc = Normalize(train_acc)\n",
    "train_gyro = Normalize(train_gyro)\n",
    "train_grav = Normalize(train_grav)\n",
    "train_linAcc = Normalize(train_linAcc)\n",
    "train_MsMag = Normalize(train_MsMag)\n",
    "train_MsAcc = Normalize(train_MsAcc)\n",
    "train_MsGyro = Normalize(train_MsGyro)\n",
    "train_jinsAcc = Normalize(train_jinsAcc)\n",
    "train_jinsGyro = Normalize(train_jinsGyro)\n",
    "\n",
    "test_acc = Normalize(test_acc)\n",
    "test_gyro = Normalize(test_gyro)\n",
    "test_grav = Normalize(test_grav)\n",
    "test_linAcc = Normalize(test_linAcc)\n",
    "test_MsMag = Normalize(test_MsMag)\n",
    "test_MsAcc = Normalize(test_MsAcc)\n",
    "test_MsGyro = Normalize(test_MsGyro)\n",
    "test_jinsAcc = Normalize(test_jinsAcc)\n",
    "test_jinsGyro = Normalize(test_jinsGyro)\n",
    "\n",
    "\n",
    "\n",
    "# # all data of shape #,400,3\n",
    "# # adding all relative data.\n",
    "# # Mobile training accelerometer + Mobile testing accelerometer data\n",
    "# train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "# train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "# train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "# train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "# train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "# train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "# train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "# train_jinsAcc_reshaped = np.append(train_jinsAcc_reshaped,test_jinsAcc_reshaped, axis=0)\n",
    "# train_jinsGyro_reshaped = np.append(train_jinsGyro_reshaped,test_jinsGyro_reshaped, axis=0)\n",
    "\n",
    "\n",
    "# print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "#                        , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "#                        train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# # all data of shape 4572,400,3\n",
    "# # 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# # stack\n",
    "# all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "#                        , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "#                        train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "# all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSample(data):\n",
    "    new_length = 80\n",
    "    old_length = data.shape[1]\n",
    "\n",
    "    x_downsampled = np.zeros((data.shape[0], new_length, 3))\n",
    "\n",
    "    for i in range(2284):\n",
    "        for j in range(3):\n",
    "            f = interp1d(np.arange(old_length), data[i, :, j], kind='linear')\n",
    "            x_downsampled[i, :, j] = f(np.linspace(0, old_length - 1, new_length))\n",
    "    \n",
    "    return x_downsampled\n",
    "            \n",
    "train_acc_reshaped = downSample(train_acc)\n",
    "train_gyro_reshaped = downSample(train_gyro)\n",
    "train_grav_reshaped = downSample(train_grav)\n",
    "train_linAcc_reshaped = downSample(train_linAcc)\n",
    "train_MsAcc_reshaped = downSample(train_MsAcc)\n",
    "train_MsGyro_reshaped = downSample(train_MsGyro)\n",
    "train_MsMag_reshaped = downSample(train_MsMag)\n",
    "\n",
    "test_acc_reshaped = downSample(test_acc)\n",
    "test_gyro_reshaped = downSample(test_gyro)\n",
    "test_grav_reshaped = downSample(test_grav)\n",
    "test_linAcc_reshaped = downSample(test_linAcc)\n",
    "test_MsAcc_reshaped = downSample(test_MsAcc)\n",
    "test_MsGyro_reshaped = downSample(test_MsGyro)\n",
    "test_MsMag_reshaped = downSample(test_MsMag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all sensors after up/down sample...  (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "# all data of shape #,80,3\n",
    "# adding all relative data.\n",
    "# Mobile training accelerometer + Mobile testing accelerometer data\n",
    "train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "train_jinsAcc_reshaped = np.append(train_jinsAcc,test_jinsAcc, axis=0)\n",
    "train_jinsGyro_reshaped = np.append(train_jinsGyro,test_jinsGyro, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "                       , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "                       train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# all data of shape 4572,400,3\n",
    "# 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# stack\n",
    "all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "                       , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "                       train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training + validation Data\n",
      "(3657, 80, 3, 9) (915, 80, 3, 9)\n",
      "Shape of training + validation Labels\n",
      "(3657,) (915,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 90% training data + labels\n",
    "# train_data = all_data[: int(all_data.shape[0]*0.9)]\n",
    "# # 10% testing data + labels\n",
    "# test_data = all_data[int(all_data.shape[0]*0.9):]\n",
    "# train_labels = all_Label[: int(all_Label.shape[0]*0.9)]\n",
    "# test_labels = all_Label[int(all_Label.shape[0]*0.9):]\n",
    "\n",
    "# print(\"\\nShape of training and testin data + labels...\\n\")\n",
    "# print(train_data.shape, test_data.shape)\n",
    "# print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(all_data, all_Label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training + validation Data\")\n",
    "print(np.shape(x_train), np.shape(x_val))\n",
    "\n",
    "# y_train = to_categorical(y_train, num_classes=55)\n",
    "# y_val = to_categorical(y_val, num_classes=55)\n",
    "\n",
    "print(\"Shape of training + validation Labels\")\n",
    "print(np.shape(y_train), np.shape(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: ------------------------------\n",
      "(80, 3, 9)\n",
      "Conv1 shape: ------------------------------\n",
      "(None, 80, 3, 50)\n",
      "Pool1 shape: ------------------------------\n",
      "(None, 40, 3, 50)\n",
      "Conv2 shape: ------------------------------\n",
      "(None, 40, 3, 40)\n",
      "Pool2 shape: ------------------------------\n",
      "(None, 14, 3, 40)\n",
      "Conv3 shape: ------------------------------\n",
      "(None, 14, 3, 30)\n",
      "Pool3 shape: ------------------------------\n",
      "(None, 7, 3, 30)\n",
      "Dense layer size: ---------------------------\n",
      "(None, 3000)\n",
      "Reshape param: -----------------------------\n",
      "1800.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [3000], output_shape = [6, 9, 30]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 3000), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 153\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_shape)      \n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m--> 153\u001b[0m model \u001b[39m=\u001b[39m conv3Autoencoder(inputShape\u001b[39m=\u001b[39;49minput_shape,\n\u001b[1;32m    154\u001b[0m                             nkerns\u001b[39m=\u001b[39;49m[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n\u001b[1;32m    155\u001b[0m                             filterSizes\u001b[39m=\u001b[39;49m[filter1Size, filter2Size, filter3Size],\n\u001b[1;32m    156\u001b[0m                             poolSizes\u001b[39m=\u001b[39;49m[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n\u001b[1;32m    157\u001b[0m                             activationConv\u001b[39m=\u001b[39;49mactivationConv,\n\u001b[1;32m    158\u001b[0m                             timeWindow\u001b[39m=\u001b[39;49mtimeWindow,\n\u001b[1;32m    159\u001b[0m                             nbSensors\u001b[39m=\u001b[39;49mnbSensors,\n\u001b[1;32m    160\u001b[0m                             activationMLP\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    161\u001b[0m                            )\n",
      "Cell \u001b[0;32mIn[7], line 111\u001b[0m, in \u001b[0;36mconv3Autoencoder\u001b[0;34m(inputShape, nkerns, filterSizes, poolSizes, activationConv, timeWindow, nbSensors, activationMLP, decoder)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReshape param: -----------------------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[39mprint\u001b[39m(outputSizeLastConv\u001b[39m*\u001b[39mnbSensors\u001b[39m*\u001b[39mnkerns[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 111\u001b[0m model\u001b[39m.\u001b[39;49madd(Reshape((\u001b[39mint\u001b[39;49m(outputSizeLastConv),\u001b[39mint\u001b[39;49m(nbSensors),\u001b[39mint\u001b[39;49m(nkerns[\u001b[39m2\u001b[39;49m]))))\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReshape shape: ------------------------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [3000], output_shape = [6, 9, 30]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 3000), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMTERS\n",
    "\n",
    "# Filter parameters, i.e. about the number of inputs processed by each neuron of the convolutional layer\n",
    "filter1Size = (11,1)\n",
    "filter2Size = (13,1)\n",
    "filter3Size = (13,1)\n",
    "\n",
    "# Downsampling factors of the pooling layers\n",
    "poolingLayer1Factor = (2,1)\n",
    "poolingLayer2Factor = (3,1)\n",
    "poolingLayer3Factor = (2,1)\n",
    "\n",
    "# Number of feature maps processed by each convolutional layer\n",
    "nbFeaturesLayer1 = 50\n",
    "nbFeaturesLayer2 = 40\n",
    "nbFeaturesLayer3 = 30\n",
    "\n",
    "# Activation function of the convolutional layer(s)\n",
    "activationConv = 'relu'\n",
    "\n",
    "# Output dimension of the LSTM\n",
    "outputLSTM = 55\n",
    "\n",
    "# Parameters of the dense layer\n",
    "activationMLP = 'relu'\n",
    "inputMLP = 100\n",
    "\n",
    "# Training parameters\n",
    "batchSize =32\n",
    "numberOfEpochs = 10\n",
    "learningRate = 0.001\n",
    "\n",
    "input_shape = (80,3,9)\n",
    "nbClasses = 55\n",
    "timeWindow = 80\n",
    "nbSensors = 9\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# convAutoencoder: define a convolutional autoencoder model with 3 conv+pooling layers\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def conv3Autoencoder(\n",
    "    inputShape,\n",
    "    nkerns,\n",
    "    filterSizes,\n",
    "    poolSizes,\n",
    "    activationConv,\n",
    "    timeWindow,\n",
    "    nbSensors,\n",
    "    activationMLP,\n",
    "    decoder=True):\n",
    "\n",
    "    #outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)/poolSizes[0][0]\n",
    "    #inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "    # NOTE: if padding = 'same', the size of the feature maps doesn't change after being convoluted\n",
    "    outputSizeLastConv = timeWindow/poolSizes[0][0]/poolSizes[1][0]/poolSizes[2][0]\n",
    "    inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    print('Input shape: ------------------------------')\n",
    "    print(inputShape)\n",
    "\n",
    "    # First convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same', input_shape=inputShape))\n",
    "    print('Conv1 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[0], padding='same'))\n",
    "    print('Pool1 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "    print('Conv2 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[1], padding='same'))\n",
    "    print('Pool2 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Third convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[2], kernel_size=filterSizes[2], activation=activationConv, padding='same'))\n",
    "    print('Conv3 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[2], padding='same'))\n",
    "    print('Pool3 shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(inputMLP, activation=activationMLP))\n",
    "    print('Dense layer size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "\n",
    "    if decoder:\n",
    "\n",
    "        # Reshaping layer\n",
    "        print('Reshape param: -----------------------------')\n",
    "        print(outputSizeLastConv*nbSensors*nkerns[2])\n",
    "        model.add(Reshape((int(outputSizeLastConv),int(nbSensors),int(nkerns[2]))))\n",
    "        print('Reshape shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # First up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[2]))\n",
    "        print('Upsampling1 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # First deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[2], kernel_size=filterSizes[2], activation=activationConv, padding='same'))\n",
    "        print('Deconv3 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[1]))\n",
    "        print('Upsampling2 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "        print('Deconv2 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Third up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[0]))\n",
    "        print('Upsampling1 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "        print('Deconv1 shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Conv2D(1, kernel_size=filterSizes[0], activation='linear', padding='same')) \n",
    "        print('Output shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)      \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = conv3Autoencoder(inputShape=input_shape,\n",
    "                            nkerns=[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n",
    "                            filterSizes=[filter1Size, filter2Size, filter3Size],\n",
    "                            poolSizes=[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n",
    "                            activationConv=activationConv,\n",
    "                            timeWindow=timeWindow,\n",
    "                            nbSensors=nbSensors,\n",
    "                            activationMLP='sigmoid'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=Adam(learning_rate=learningRate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# model.fit(x=x_train, y=x_train, epochs=2, batch_size=batchSize, validation_data=(y_train, y_train))\n",
    "model.fit(all_data, all_Label, epochs=2, batch_size=batchSize, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train_comp = model.predict(x_train)\n",
    "prediction_test_comp = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# x_train_compressed = model.predict(train_data)\n",
    "# x_test_compressed = model.predict(test_data)\n",
    "\n",
    "predict_train_comp = np.reshape(predict_train_comp, (predict_train_comp.shape[0], 80*9))\n",
    "# Train an SVM classifier on the compressed data\n",
    "svm = SVC(kernel='linear', C=1, gamma='auto')\n",
    "svm.fit(predict_train_comp, y_train)\n",
    "\n",
    "# print(x_train_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the SVM classifier\n",
    "prediction_test_comp = np.reshape(prediction_test_comp, (prediction_test_comp.shape[0], 80*9))\n",
    "svm_score = svm.score(prediction_test_comp, y_val)\n",
    "print(\"Accuracy: %.2f%%\" % (svm_score*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
