{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\"\"\"# Loading Data\"\"\"\n",
    "\n",
    "path = \"/FYP/Cog_DataSets/testing/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/testing/\"\n",
    "test_acc = np.load(path+\"testAccelerometer.npy\")\n",
    "test_grav = np.load(path+\"testGravity.npy\")\n",
    "test_gyro = np.load(path+\"testGyroscope.npy\")\n",
    "test_jinsAcc = np.load(path+\"testJinsAccelerometer.npy\")\n",
    "test_jinsGyro = np.load(path+\"testJinsGyroscope.npy\")\n",
    "test_Label =np.load(path+\"testLabels.npy\") \n",
    "test_linAcc = np.load(path+\"testLinearAcceleration.npy\")\n",
    "test_MsAcc = np.load(path+\"testMSAccelerometer.npy\")\n",
    "test_MsGyro = np.load(path + \"testMSGyroscope.npy\")\n",
    "test_MsMag = np.load(path+\"testMagnetometer.npy\")\n",
    "# test_acc\n",
    "\n",
    "path = \"/FYP/Cog_DataSets/training/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/training/\"\n",
    "train_acc = np.load(path+\"trainAccelerometer.npy\")\n",
    "train_grav = np.load(path+\"trainGravity.npy\")\n",
    "train_gyro = np.load(path+\"trainGyroscope.npy\")\n",
    "train_jinsAcc = np.load(path+\"trainJinsAccelerometer.npy\")\n",
    "train_jinsGyro = np.load(path+\"trainJinsGyroscope.npy\")\n",
    "train_Label =np.load(path+\"trainLabels.npy\") \n",
    "train_linAcc = np.load(path+\"trainLinearAcceleration.npy\")\n",
    "train_MsAcc = np.load(path+\"trainMSAccelerometer.npy\")\n",
    "train_MsGyro = np.load(path + \"trainMSGyroscope.npy\")\n",
    "train_MsMag = np.load(path+\"trainMagnetometer.npy\")\n",
    "# print(train_Label.shape)\n",
    "\n",
    "\n",
    "def Normalize(X):\n",
    "  norm = []\n",
    "  for I in range(len(X)):\n",
    "    norm.append(normalize(X[I]))\n",
    "  norm=np.array(norm)\n",
    "  return norm\n",
    "\n",
    "train_acc = Normalize(train_acc)\n",
    "train_gyro = Normalize(train_gyro)\n",
    "train_grav = Normalize(train_grav)\n",
    "train_linAcc = Normalize(train_linAcc)\n",
    "train_MsMag = Normalize(train_MsMag)\n",
    "train_MsAcc = Normalize(train_MsAcc)\n",
    "train_MsGyro = Normalize(train_MsGyro)\n",
    "train_jinsAcc = Normalize(train_jinsAcc)\n",
    "train_jinsGyro = Normalize(train_jinsGyro)\n",
    "\n",
    "test_acc = Normalize(test_acc)\n",
    "test_gyro = Normalize(test_gyro)\n",
    "test_grav = Normalize(test_grav)\n",
    "test_linAcc = Normalize(test_linAcc)\n",
    "test_MsMag = Normalize(test_MsMag)\n",
    "test_MsAcc = Normalize(test_MsAcc)\n",
    "test_MsGyro = Normalize(test_MsGyro)\n",
    "test_jinsAcc = Normalize(test_jinsAcc)\n",
    "test_jinsGyro = Normalize(test_jinsGyro)\n",
    "\n",
    "\n",
    "\n",
    "# # all data of shape #,400,3\n",
    "# # adding all relative data.\n",
    "# # Mobile training accelerometer + Mobile testing accelerometer data\n",
    "# train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "# train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "# train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "# train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "# train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "# train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "# train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "# train_jinsAcc_reshaped = np.append(train_jinsAcc_reshaped,test_jinsAcc_reshaped, axis=0)\n",
    "# train_jinsGyro_reshaped = np.append(train_jinsGyro_reshaped,test_jinsGyro_reshaped, axis=0)\n",
    "\n",
    "\n",
    "# print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "#                        , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "#                        train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# # all data of shape 4572,400,3\n",
    "# # 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# # stack\n",
    "# all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "#                        , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "#                        train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "# all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSample(data):\n",
    "    new_length = 80\n",
    "    old_length = data.shape[1]\n",
    "\n",
    "    x_downsampled = np.zeros((data.shape[0], new_length, 3))\n",
    "\n",
    "    for i in range(2284):\n",
    "        for j in range(3):\n",
    "            f = interp1d(np.arange(old_length), data[i, :, j], kind='linear')\n",
    "            x_downsampled[i, :, j] = f(np.linspace(0, old_length - 1, new_length))\n",
    "    \n",
    "    return x_downsampled\n",
    "            \n",
    "train_acc_reshaped = downSample(train_acc)\n",
    "train_gyro_reshaped = downSample(train_gyro)\n",
    "train_grav_reshaped = downSample(train_grav)\n",
    "train_linAcc_reshaped = downSample(train_linAcc)\n",
    "train_MsAcc_reshaped = downSample(train_MsAcc)\n",
    "train_MsGyro_reshaped = downSample(train_MsGyro)\n",
    "train_MsMag_reshaped = downSample(train_MsMag)\n",
    "\n",
    "test_acc_reshaped = downSample(test_acc)\n",
    "test_gyro_reshaped = downSample(test_gyro)\n",
    "test_grav_reshaped = downSample(test_grav)\n",
    "test_linAcc_reshaped = downSample(test_linAcc)\n",
    "test_MsAcc_reshaped = downSample(test_MsAcc)\n",
    "test_MsGyro_reshaped = downSample(test_MsGyro)\n",
    "test_MsMag_reshaped = downSample(test_MsMag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all sensors after up/down sample...  (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3) (4572, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "# all data of shape #,80,3\n",
    "# adding all relative data.\n",
    "# Mobile training accelerometer + Mobile testing accelerometer data\n",
    "train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "train_jinsAcc_reshaped = np.append(train_jinsAcc,test_jinsAcc, axis=0)\n",
    "train_jinsGyro_reshaped = np.append(train_jinsGyro,test_jinsGyro, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "                       , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "                       train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# all data of shape 4572,400,3\n",
    "# 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# stack\n",
    "all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "                       , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "                       train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training + validation Data\n",
      "(3657, 80, 3, 9) (915, 80, 3, 9)\n",
      "Shape of training + validation Labels\n",
      "(3657,) (915,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 90% training data + labels\n",
    "# train_data = all_data[: int(all_data.shape[0]*0.9)]\n",
    "# # 10% testing data + labels\n",
    "# test_data = all_data[int(all_data.shape[0]*0.9):]\n",
    "# train_labels = all_Label[: int(all_Label.shape[0]*0.9)]\n",
    "# test_labels = all_Label[int(all_Label.shape[0]*0.9):]\n",
    "\n",
    "# print(\"\\nShape of training and testin data + labels...\\n\")\n",
    "# print(train_data.shape, test_data.shape)\n",
    "# print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(all_data, all_Label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training + validation Data\")\n",
    "print(np.shape(x_train), np.shape(x_val))\n",
    "\n",
    "# y_train = to_categorical(y_train, num_classes=55)\n",
    "# y_val = to_categorical(y_val, num_classes=55)\n",
    "\n",
    "print(\"Shape of training + validation Labels\")\n",
    "print(np.shape(y_train), np.shape(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: ------------------------------\n",
      "(80, 3, 9)\n",
      "Conv shape: ------------------------------\n",
      "(None, 80, 3, 50)\n",
      "Pool shape: ------------------------------\n",
      "(None, 40, 3, 50)\n",
      "Dense layer size: ---------------------------\n",
      "(None, 18000)\n",
      "Reshape param: -----------------------------\n",
      "18000.0\n",
      "Reshape shape: ------------------------------\n",
      "(None, 40, 9, 50)\n",
      "Upsampling shape: ------------------------------\n",
      "(None, 80, 9, 50)\n",
      "Deconv shape: ------------------------------\n",
      "(None, 80, 9, 50)\n",
      "Output shape: ------------------------------\n",
      "(None, 80, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMTERS\n",
    "\n",
    "# Filter parameters, i.e. about the number of inputs processed by each neuron of the convolutional layer\n",
    "filter1Size = (11,1)\n",
    "filter2Size = (13,1)\n",
    "filter3Size = (13,1)\n",
    "\n",
    "# Downsampling factors of the pooling layers\n",
    "poolingLayer1Factor = (2,1)\n",
    "poolingLayer2Factor = (3,1)\n",
    "poolingLayer3Factor = (2,1)\n",
    "\n",
    "# Number of feature maps processed by each convolutional layer\n",
    "nbFeaturesLayer1 = 50\n",
    "nbFeaturesLayer2 = 40\n",
    "nbFeaturesLayer3 = 30\n",
    "\n",
    "# Activation function of the convolutional layer(s)\n",
    "activationConv = 'relu'\n",
    "\n",
    "# Output dimension of the LSTM\n",
    "outputLSTM = 55\n",
    "\n",
    "# Parameters of the dense layer\n",
    "activationMLP = 'relu'\n",
    "inputMLP = 100\n",
    "\n",
    "# Training parameters\n",
    "batchSize =32\n",
    "numberOfEpochs = 10\n",
    "learningRate = 0.001\n",
    "\n",
    "input_shape = (80,3,9)\n",
    "nbClasses = 55\n",
    "timeWindow = 80\n",
    "nbSensors = 9\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# convAutoencoder: define a convolutional autoencoder model with conv+pooling layers\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def convAutoencoder(\n",
    "    inputShape,\n",
    "    nkerns,\n",
    "    filterSizes,\n",
    "    poolSizes,\n",
    "    activationConv,\n",
    "    timeWindow,\n",
    "    nbSensors,\n",
    "    activationMLP,\n",
    "    decoder=True):\n",
    "\n",
    "    #outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)/poolSizes[0][0]\n",
    "    #inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "    # NOTE: if padding = 'same', the size of the feature maps doesn't change after being convoluted\n",
    "    outputSizeLastConv = timeWindow/poolSizes[0][0]\n",
    "    inputMLP = nkerns[0]*outputSizeLastConv*nbSensors\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    print('Input shape: ------------------------------')\n",
    "    print(inputShape)\n",
    "\n",
    "    # First convolutional layer\n",
    "    # Note: default padding = zero padding?\n",
    "    model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same', input_shape=inputShape))\n",
    "    print('Conv shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[0], padding='same'))\n",
    "    print('Pool shape: ------------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    #model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(inputMLP, activation=activationMLP))\n",
    "    print('Dense layer size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "    print('Reshape param: -----------------------------')\n",
    "    print(outputSizeLastConv*nbSensors*nkerns[0])\n",
    "\n",
    "\n",
    "    if decoder:\n",
    "\n",
    "        # Reshaping layer\n",
    "        model.add(Reshape((int(outputSizeLastConv),int(nbSensors),int(nkerns[0]))))\n",
    "        print('Reshape shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # First deconvolutional layer\n",
    "        #model.add(Conv2D(nkerns[1], kernel_size=filterSizes[1], activation=activationConv, padding='same'))\n",
    "\n",
    "        # Up-sampling layer\n",
    "        model.add(UpSampling2D(size=poolSizes[0]))\n",
    "        print('Upsampling shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Second deconvolutional layer\n",
    "        model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv, padding='same'))\n",
    "        print('Deconv shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Conv2D(1, kernel_size=filterSizes[0], activation='linear', padding='same')) \n",
    "        print('Output shape: ------------------------------')\n",
    "        print(model.layers[-1].output_shape)      \n",
    "\n",
    "    return model\n",
    "\n",
    "model = convAutoencoder(inputShape=input_shape,\n",
    "                            nkerns=[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n",
    "                            filterSizes=[filter1Size, filter2Size, filter3Size],\n",
    "                            poolSizes=[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n",
    "                            activationConv=activationConv,\n",
    "                            timeWindow=timeWindow,\n",
    "                            nbSensors=nbSensors,\n",
    "                            activationMLP='sigmoid'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "115/115 [==============================] - 249s 2s/step - loss: 317.0268 - accuracy: 0.0205 - val_loss: 253.1517 - val_accuracy: 0.0208\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 223s 2s/step - loss: 260.4399 - accuracy: 0.0205 - val_loss: 250.6216 - val_accuracy: 0.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8c092dd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=Adam(learning_rate=learningRate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# model.fit(x=x_train, y=x_train, epochs=2, batch_size=batchSize, validation_data=(y_train, y_train))\n",
    "model.fit(all_data, all_Label, epochs=2, batch_size=batchSize, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 12s 105ms/step\n",
      "29/29 [==============================] - 3s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_train_comp = model.predict(x_train)\n",
    "prediction_test_comp = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 80, 3, 50)         5000      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 40, 3, 50)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18000)             108018000 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 40, 9, 50)         0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 80, 9, 50)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 80, 9, 50)         27550     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 80, 9, 1)          551       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,051,101\n",
      "Trainable params: 108,051,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=&#x27;auto&#x27;, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=&#x27;auto&#x27;, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# x_train_compressed = model.predict(train_data)\n",
    "# x_test_compressed = model.predict(test_data)\n",
    "\n",
    "predict_train_comp = np.reshape(predict_train_comp, (predict_train_comp.shape[0], 80*9))\n",
    "# Train an SVM classifier on the compressed data\n",
    "svm = SVC(kernel='linear', C=1, gamma='auto')\n",
    "svm.fit(predict_train_comp, y_train)\n",
    "\n",
    "# print(x_train_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.64%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the SVM classifier\n",
    "prediction_test_comp = np.reshape(prediction_test_comp, (prediction_test_comp.shape[0], 80*9))\n",
    "svm_score = svm.score(prediction_test_comp, y_val)\n",
    "print(\"Accuracy: %.2f%%\" % (svm_score*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"# Reshape and stack Data Before Fitting to Model\"\"\"\n",
    "# # changing shape of sensor data to (#,400,3)\n",
    "# # downsample\n",
    "# train_acc_reshaped = block_reduce(train_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "# train_gyro_reshaped = block_reduce(train_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "# train_grav_reshaped = block_reduce(train_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "# train_linAcc_reshaped = block_reduce(train_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "# # upsample\n",
    "# train_MsAcc_reshaped = resize(train_MsAcc, (2284, 400, 3), mode='edge')\n",
    "# train_MsGyro_reshaped = resize(train_MsGyro, (2284, 400, 3), mode='edge')\n",
    "# # upsample\n",
    "# train_MsMag_reshaped = np.repeat(train_MsMag, 2, axis=1)\n",
    "# train_jinsAcc_reshaped = np.repeat(train_jinsAcc, 5, axis=1)\n",
    "# train_jinsGyro_reshaped = np.repeat(train_jinsGyro, 5, axis=1)\n",
    "\n",
    "# test_acc_reshaped = block_reduce(test_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "# test_gyro_reshaped = block_reduce(test_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "# test_grav_reshaped = block_reduce(test_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "# test_linAcc_reshaped = block_reduce(test_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "# test_MsAcc_reshaped = resize(test_MsAcc, (2288, 400, 3), mode='edge')\n",
    "# test_MsGyro_reshaped = resize(test_MsGyro, (2288, 400, 3), mode='edge')\n",
    "# test_MsMag_reshaped = np.repeat(test_MsMag, 2, axis=1)\n",
    "# test_jinsAcc_reshaped = np.repeat(test_jinsAcc, 5, axis=1)\n",
    "# test_jinsGyro_reshaped = np.repeat(test_jinsGyro, 5, axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
