{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 15:23:01.220280: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 15:23:01.302861: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 15:23:01.303846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 15:23:02.397732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Importing Libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import LSTM\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Loading Data\"\"\"\n",
    "\n",
    "path = \"../../Cog_DataSets/testing/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/testing/\"\n",
    "test_acc = np.load(path+\"testAccelerometer.npy\")\n",
    "test_grav = np.load(path+\"testGravity.npy\")\n",
    "test_gyro = np.load(path+\"testGyroscope.npy\")\n",
    "test_jinsAcc = np.load(path+\"testJinsAccelerometer.npy\")\n",
    "test_jinsGyro = np.load(path+\"testJinsGyroscope.npy\")\n",
    "test_Label =np.load(path+\"testLabels.npy\") \n",
    "test_linAcc = np.load(path+\"testLinearAcceleration.npy\")\n",
    "test_MsAcc = np.load(path+\"testMSAccelerometer.npy\")\n",
    "test_MsGyro = np.load(path + \"testMSGyroscope.npy\")\n",
    "test_MsMag = np.load(path+\"testMagnetometer.npy\")\n",
    "# test_acc\n",
    "\n",
    "path = \"../../Cog_DataSets/training/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/training/\"\n",
    "train_acc = np.load(path+\"trainAccelerometer.npy\")\n",
    "train_grav = np.load(path+\"trainGravity.npy\")\n",
    "train_gyro = np.load(path+\"trainGyroscope.npy\")\n",
    "train_jinsAcc = np.load(path+\"trainJinsAccelerometer.npy\")\n",
    "train_jinsGyro = np.load(path+\"trainJinsGyroscope.npy\")\n",
    "train_Label =np.load(path+\"trainLabels.npy\") \n",
    "train_linAcc = np.load(path+\"trainLinearAcceleration.npy\")\n",
    "train_MsAcc = np.load(path+\"trainMSAccelerometer.npy\")\n",
    "train_MsGyro = np.load(path + \"trainMSGyroscope.npy\")\n",
    "train_MsMag = np.load(path+\"trainMagnetometer.npy\")\n",
    "# print(train_Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all sensors after up/down sample...  (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Reshape and stack Data Before Fitting to Model\"\"\"\n",
    "# changing shape of sensor data to (#,400,3)\n",
    "# downsample\n",
    "train_acc_reshaped = block_reduce(train_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "train_gyro_reshaped = block_reduce(train_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "train_grav_reshaped = block_reduce(train_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "train_linAcc_reshaped = block_reduce(train_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "# upsample\n",
    "train_MsAcc_reshaped = resize(train_MsAcc, (2284, 400, 3), mode='edge')\n",
    "train_MsGyro_reshaped = resize(train_MsGyro, (2284, 400, 3), mode='edge')\n",
    "# upsample\n",
    "train_MsMag_reshaped = np.repeat(train_MsMag, 2, axis=1)\n",
    "train_jinsAcc_reshaped = np.repeat(train_jinsAcc, 5, axis=1)\n",
    "train_jinsGyro_reshaped = np.repeat(train_jinsGyro, 5, axis=1)\n",
    "\n",
    "test_acc_reshaped = block_reduce(test_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "test_gyro_reshaped = block_reduce(test_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "test_grav_reshaped = block_reduce(test_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "test_linAcc_reshaped = block_reduce(test_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "test_MsAcc_reshaped = resize(test_MsAcc, (2288, 400, 3), mode='edge')\n",
    "test_MsGyro_reshaped = resize(test_MsGyro, (2288, 400, 3), mode='edge')\n",
    "test_MsMag_reshaped = np.repeat(test_MsMag, 2, axis=1)\n",
    "test_jinsAcc_reshaped = np.repeat(test_jinsAcc, 5, axis=1)\n",
    "test_jinsGyro_reshaped = np.repeat(test_jinsGyro, 5, axis=1)\n",
    "\n",
    "# all data of shape #,400,3\n",
    "# adding all relative data.\n",
    "# Mobile training accelerometer + Mobile testing accelerometer data\n",
    "train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "train_jinsAcc_reshaped = np.append(train_jinsAcc_reshaped,test_jinsAcc_reshaped, axis=0)\n",
    "train_jinsGyro_reshaped = np.append(train_jinsGyro_reshaped,test_jinsGyro_reshaped, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "                       , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "                       train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# all data of shape 4572,400,3\n",
    "# 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# stack\n",
    "all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "                       , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "                       train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of training and testin data + labels...\n",
      "\n",
      "(4114, 400, 3, 9) (458, 400, 3, 9)\n",
      "(4114,) (458,)\n",
      "Shape of training + validation Data\n",
      "(3291, 400, 3, 9) (3291,)\n",
      "Shape of training + validation Labels\n",
      "(3291, 55) (823, 55)\n"
     ]
    }
   ],
   "source": [
    "# 90% training data + labels\n",
    "train_data = all_data[: int(all_data.shape[0]*0.9)]\n",
    "# 10% testing data + labels\n",
    "test_data = all_data[int(all_data.shape[0]*0.9):]\n",
    "train_labels = all_Label[: int(all_Label.shape[0]*0.9)]\n",
    "test_labels = all_Label[int(all_Label.shape[0]*0.9):]\n",
    "\n",
    "print(\"\\nShape of training and testin data + labels...\\n\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training + validation Data\")\n",
    "print(np.shape(x_train), np.shape(y_train))\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=55)\n",
    "y_val = to_categorical(y_val, num_classes=55)\n",
    "\n",
    "print(\"Shape of training + validation Labels\")\n",
    "print(np.shape(y_train), np.shape(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [195, 3, 50], output_shape = [195, 450]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 195, 3, 50), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39m# Return the model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 95\u001b[0m model \u001b[39m=\u001b[39m normConv1Lstm(\n\u001b[1;32m     96\u001b[0m     inputShape\u001b[39m=\u001b[39;49m(\u001b[39m400\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m9\u001b[39;49m),\n\u001b[1;32m     97\u001b[0m nkerns\u001b[39m=\u001b[39;49m[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n\u001b[1;32m     98\u001b[0m filterSizes\u001b[39m=\u001b[39;49m[filter1Size, filter2Size, filter3Size],\n\u001b[1;32m     99\u001b[0m poolSizes\u001b[39m=\u001b[39;49m[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n\u001b[1;32m    100\u001b[0m activationConv\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    101\u001b[0m timeWindow\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m,\n\u001b[1;32m    102\u001b[0m nbSensors\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m,\n\u001b[1;32m    103\u001b[0m outputLSTM\u001b[39m=\u001b[39;49moutputLSTM,\n\u001b[1;32m    104\u001b[0m inputMLP\u001b[39m=\u001b[39;49minputMLP,\n\u001b[1;32m    105\u001b[0m activationMLP\u001b[39m=\u001b[39;49mactivationMLP,\n\u001b[1;32m    106\u001b[0m nbClasses\u001b[39m=\u001b[39;49m\u001b[39m55\u001b[39;49m,\n\u001b[1;32m    107\u001b[0m withSoftmax\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[10], line 74\u001b[0m, in \u001b[0;36mnormConv1Lstm\u001b[0;34m(inputShape, nkerns, filterSizes, poolSizes, activationConv, timeWindow, nbSensors, outputLSTM, inputMLP, activationMLP, nbClasses, withSoftmax)\u001b[0m\n\u001b[1;32m     65\u001b[0m model\u001b[39m.\u001b[39madd(MaxPooling2D(pool_size\u001b[39m=\u001b[39mpoolSizes[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     67\u001b[0m \u001b[39m#model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation='linear', input_shape=inputShape))\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m#model.add(BatchNormalization())\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m#model.add(Activation(activationConv))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# LSTM layer with a many-to-one implementation\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# Note: size of the output = (outputSizeLastConv, outputLSTM)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m model\u001b[39m.\u001b[39;49madd(Reshape((outputSizeLastConv,nbSensors\u001b[39m*\u001b[39;49mnkerns[\u001b[39m0\u001b[39;49m]))) \n\u001b[1;32m     75\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(outputLSTM,return_sequences\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m     76\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLSTM output size: ---------------------------\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [195, 3, 50], output_shape = [195, 450]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 195, 3, 50), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMTERS\n",
    "\n",
    "# Filter parameters, i.e. about the number of inputs processed by each neuron of the convolutional layer\n",
    "filter1Size = (11,1)\n",
    "filter2Size = (13,1)\n",
    "filter3Size = (13,1)\n",
    "\n",
    "# Downsampling factors of the pooling layers\n",
    "poolingLayer1Factor = (2,1)\n",
    "poolingLayer2Factor = (3,1)\n",
    "poolingLayer3Factor = (2,1)\n",
    "\n",
    "# Number of feature maps processed by each convolutional layer\n",
    "nbFeaturesLayer1 = 50\n",
    "nbFeaturesLayer2 = 40\n",
    "nbFeaturesLayer3 = 30\n",
    "\n",
    "# Activation function of the convolutional layer(s)\n",
    "activationConv = 'relu'\n",
    "\n",
    "# Output dimension of the LSTM\n",
    "outputLSTM = 100\n",
    "\n",
    "# Parameters of the dense layer\n",
    "activationMLP = 'relu'\n",
    "inputMLP = 2500\n",
    "\n",
    "# Training parameters\n",
    "batchSize = 400\n",
    "numberOfEpochs = 30\n",
    "learningRate = 0.001\n",
    "\n",
    "input_shape = (400,3,9)\n",
    "nbClasses = 55\n",
    "timeWindow = 400\n",
    "nbSensors = 9\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# normConv1Lstm: define a batch normalization + 2 convolutional/max-pooling + LSTM DNN\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def normConv1Lstm(\n",
    "    inputShape,\n",
    "    nkerns,\n",
    "    filterSizes,\n",
    "    poolSizes,\n",
    "    activationConv,\n",
    "    timeWindow,\n",
    "    nbSensors,\n",
    "    outputLSTM,\n",
    "    inputMLP,\n",
    "    activationMLP,\n",
    "    nbClasses,\n",
    "    withSoftmax=True):\n",
    "\n",
    "    outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)/poolSizes[0][0]\n",
    "    #outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Batch normalization layer\n",
    "    model.add(BatchNormalization(input_shape=inputShape))\n",
    "\n",
    "    # Convolution + max-pooling layers\n",
    "    model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv))\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[0]))\n",
    "\n",
    "    #model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation='linear', input_shape=inputShape))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activationConv))\n",
    "    #model.add(MaxPooling2D(pool_size=poolSizes[0]))\n",
    "\n",
    "    # LSTM layer with a many-to-one implementation\n",
    "    # Note: size of the output = (outputSizeLastConv, outputLSTM)\n",
    "    model.add(Reshape((outputSizeLastConv,nbSensors*nkerns[0]))) \n",
    "    model.add(LSTM(outputLSTM,return_sequences=False))\n",
    "    print('LSTM output size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Fully-connected layer\n",
    "    model.add(Dense(inputMLP, activation=activationMLP))\n",
    "    print('Dense layer size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    # Softmax layer\n",
    "    if withSoftmax:\n",
    "        model.add(Dense(nbClasses, activation='softmax'))\n",
    "\n",
    "    # Print the summary of the model\n",
    "    model.summary\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "model = normConv1Lstm(\n",
    "    inputShape=(400,3,9),\n",
    "nkerns=[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n",
    "filterSizes=[filter1Size, filter2Size, filter3Size],\n",
    "poolSizes=[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n",
    "activationConv='relu',\n",
    "timeWindow=400,\n",
    "nbSensors=9,\n",
    "outputLSTM=outputLSTM,\n",
    "inputMLP=inputMLP,\n",
    "activationMLP=activationMLP,\n",
    "nbClasses=55,\n",
    "withSoftmax=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
