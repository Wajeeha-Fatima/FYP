{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Importing Libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import LSTM\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Loading Data\"\"\"\n",
    "\n",
    "path = \"../../Cog_DataSets/testing/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/testing/\"\n",
    "test_acc = np.load(path+\"testAccelerometer.npy\")\n",
    "test_grav = np.load(path+\"testGravity.npy\")\n",
    "test_gyro = np.load(path+\"testGyroscope.npy\")\n",
    "test_jinsAcc = np.load(path+\"testJinsAccelerometer.npy\")\n",
    "test_jinsGyro = np.load(path+\"testJinsGyroscope.npy\")\n",
    "test_Label =np.load(path+\"testLabels.npy\") \n",
    "test_linAcc = np.load(path+\"testLinearAcceleration.npy\")\n",
    "test_MsAcc = np.load(path+\"testMSAccelerometer.npy\")\n",
    "test_MsGyro = np.load(path + \"testMSGyroscope.npy\")\n",
    "test_MsMag = np.load(path+\"testMagnetometer.npy\")\n",
    "# test_acc\n",
    "\n",
    "path = \"../../Cog_DataSets/training/\"\n",
    "# path = \"drive/MyDrive/Colab Notebooks/CogAge/Datasets/training/\"\n",
    "train_acc = np.load(path+\"trainAccelerometer.npy\")\n",
    "train_grav = np.load(path+\"trainGravity.npy\")\n",
    "train_gyro = np.load(path+\"trainGyroscope.npy\")\n",
    "train_jinsAcc = np.load(path+\"trainJinsAccelerometer.npy\")\n",
    "train_jinsGyro = np.load(path+\"trainJinsGyroscope.npy\")\n",
    "train_Label =np.load(path+\"trainLabels.npy\") \n",
    "train_linAcc = np.load(path+\"trainLinearAcceleration.npy\")\n",
    "train_MsAcc = np.load(path+\"trainMSAccelerometer.npy\")\n",
    "train_MsGyro = np.load(path + \"trainMSGyroscope.npy\")\n",
    "train_MsMag = np.load(path+\"trainMagnetometer.npy\")\n",
    "# print(train_Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all sensors after up/down sample...  (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3) (4572, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Reshape and stack Data Before Fitting to Model\"\"\"\n",
    "# changing shape of sensor data to (#,400,3)\n",
    "# downsample\n",
    "train_acc_reshaped = block_reduce(train_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "train_gyro_reshaped = block_reduce(train_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "train_grav_reshaped = block_reduce(train_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "train_linAcc_reshaped = block_reduce(train_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "# upsample\n",
    "train_MsAcc_reshaped = resize(train_MsAcc, (2284, 400, 3), mode='edge')\n",
    "train_MsGyro_reshaped = resize(train_MsGyro, (2284, 400, 3), mode='edge')\n",
    "# upsample\n",
    "train_MsMag_reshaped = np.repeat(train_MsMag, 2, axis=1)\n",
    "train_jinsAcc_reshaped = np.repeat(train_jinsAcc, 5, axis=1)\n",
    "train_jinsGyro_reshaped = np.repeat(train_jinsGyro, 5, axis=1)\n",
    "\n",
    "test_acc_reshaped = block_reduce(test_acc, block_size=(1, 2, 1), func=np.mean)\n",
    "test_gyro_reshaped = block_reduce(test_gyro, block_size=(1, 2, 1), func=np.mean)\n",
    "test_grav_reshaped = block_reduce(test_grav, block_size=(1, 2, 1), func=np.mean)\n",
    "test_linAcc_reshaped = block_reduce(test_linAcc, block_size=(1, 2, 1), func=np.mean)\n",
    "test_MsAcc_reshaped = resize(test_MsAcc, (2288, 400, 3), mode='edge')\n",
    "test_MsGyro_reshaped = resize(test_MsGyro, (2288, 400, 3), mode='edge')\n",
    "test_MsMag_reshaped = np.repeat(test_MsMag, 2, axis=1)\n",
    "test_jinsAcc_reshaped = np.repeat(test_jinsAcc, 5, axis=1)\n",
    "test_jinsGyro_reshaped = np.repeat(test_jinsGyro, 5, axis=1)\n",
    "\n",
    "# all data of shape #,400,3\n",
    "# adding all relative data.\n",
    "# Mobile training accelerometer + Mobile testing accelerometer data\n",
    "train_acc_reshaped = np.append(train_acc_reshaped,test_acc_reshaped, axis=0)\n",
    "train_gyro_reshaped = np.append(train_gyro_reshaped,test_gyro_reshaped, axis=0)\n",
    "train_grav_reshaped = np.append(train_grav_reshaped,test_grav_reshaped, axis=0)\n",
    "train_linAcc_reshaped = np.append(train_linAcc_reshaped,test_linAcc_reshaped, axis=0)\n",
    "train_MsAcc_reshaped = np.append(train_MsAcc_reshaped,test_MsAcc_reshaped, axis=0)\n",
    "train_MsGyro_reshaped = np.append(train_MsGyro_reshaped,test_MsGyro_reshaped, axis=0)\n",
    "train_MsMag_reshaped = np.append(train_MsMag_reshaped,test_MsMag_reshaped, axis=0)\n",
    "train_jinsAcc_reshaped = np.append(train_jinsAcc_reshaped,test_jinsAcc_reshaped, axis=0)\n",
    "train_jinsGyro_reshaped = np.append(train_jinsGyro_reshaped,test_jinsGyro_reshaped, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of all sensors after up/down sample... \", train_acc_reshaped.shape, train_gyro_reshaped.shape, train_grav_reshaped.shape, train_linAcc_reshaped.shape\n",
    "                       , train_MsAcc_reshaped.shape, train_MsGyro_reshaped.shape, train_MsMag_reshaped.shape,\n",
    "                       train_jinsAcc_reshaped.shape, train_jinsGyro_reshaped.shape)\n",
    "\n",
    "# all data of shape 4572,400,3\n",
    "# 4572 = 2284(training) + 2288(testing)\n",
    "\n",
    "\n",
    "# stack\n",
    "all_data = np.stack([train_acc_reshaped, train_gyro_reshaped, train_grav_reshaped, train_linAcc_reshaped\n",
    "                       , train_MsAcc_reshaped, train_MsGyro_reshaped, train_MsMag_reshaped,\n",
    "                       train_jinsAcc_reshaped, train_jinsGyro_reshaped], axis=-1)\n",
    "\n",
    "all_Label = np.append(train_Label, test_Label, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of training and testin data + labels...\n",
      "\n",
      "(3200, 400, 3, 9) (1372, 400, 3, 9)\n",
      "(3200,) (1372,)\n",
      "Shape of training + validation Data\n",
      "(2560, 400, 3, 9) (2560,)\n",
      "Shape of training + validation Labels\n",
      "(2560, 55) (640, 55)\n"
     ]
    }
   ],
   "source": [
    "# 70% training data + labels\n",
    "train_data = all_data[: int(all_data.shape[0]*0.7)]\n",
    "# 30% testing data + labels\n",
    "test_data = all_data[int(all_data.shape[0]*0.7):]\n",
    "train_labels = all_Label[: int(all_Label.shape[0]*0.7)]\n",
    "test_labels = all_Label[int(all_Label.shape[0]*0.7):]\n",
    "\n",
    "print(\"\\nShape of training and testin data + labels...\\n\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training + validation Data\")\n",
    "print(np.shape(x_train), np.shape(y_train))\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=55)\n",
    "y_val = to_categorical(y_val, num_classes=55)\n",
    "\n",
    "print(\"Shape of training + validation Labels\")\n",
    "print(np.shape(y_train), np.shape(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 195, 3, 50)\n",
      "LSTM output size: ---------------------------\n",
      "(None, 100)\n",
      "Dense layer size: ---------------------------\n",
      "(None, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:08:51.206410: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-09 17:08:51.210573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-09 17:08:51.213741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMTERS\n",
    "\n",
    "# Filter parameters, i.e. about the number of inputs processed by each neuron of the convolutional layer\n",
    "filter1Size = (11,1)\n",
    "filter2Size = (13,1)\n",
    "filter3Size = (13,1)\n",
    "\n",
    "# Downsampling factors of the pooling layers\n",
    "poolingLayer1Factor = (2,1)\n",
    "poolingLayer2Factor = (3,1)\n",
    "poolingLayer3Factor = (2,1)\n",
    "\n",
    "# Number of feature maps processed by each convolutional layer\n",
    "nbFeaturesLayer1 = 50\n",
    "nbFeaturesLayer2 = 40\n",
    "nbFeaturesLayer3 = 30\n",
    "\n",
    "# Activation function of the convolutional layer(s)\n",
    "activationConv = 'relu'\n",
    "\n",
    "# Output dimension of the LSTM\n",
    "outputLSTM = 100\n",
    "\n",
    "# Parameters of the dense layer\n",
    "activationMLP = 'relu'\n",
    "inputMLP = 500\n",
    "\n",
    "# Training parameters\n",
    "batchSize = 400\n",
    "numberOfEpochs = 30\n",
    "learningRate = 0.001\n",
    "\n",
    "input_shape = (400,3,9)\n",
    "nbClasses = 55\n",
    "timeWindow = 400\n",
    "nbSensors = 9\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# normConv1Lstm: define a batch normalization + 2 convolutional/max-pooling + LSTM DNN\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def normConv1Lstm(\n",
    "    inputShape,\n",
    "    nkerns,\n",
    "    filterSizes,\n",
    "    poolSizes,\n",
    "    activationConv,\n",
    "    timeWindow,\n",
    "    nbSensors,\n",
    "    outputLSTM,\n",
    "    inputMLP,\n",
    "    activationMLP,\n",
    "    nbClasses,\n",
    "    withSoftmax=True):\n",
    "\n",
    "    outputSizeLastConv = int((timeWindow-filterSizes[0][0]+1)/poolSizes[0][0])\n",
    "    #outputSizeLastConv = (timeWindow-filterSizes[0][0]+1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Batch normalization layer\n",
    "    model.add(BatchNormalization(input_shape=inputShape))\n",
    "\n",
    "    # Convolution + max-pooling layers\n",
    "    model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation=activationConv))\n",
    "    model.add(MaxPooling2D(pool_size=poolSizes[0]))\n",
    "\n",
    "    # model.add(Conv2D(nkerns[0], kernel_size=filterSizes[0], activation='linear', input_shape=inputShape))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Activation(activationConv))\n",
    "    # model.add(MaxPooling2D(pool_size=poolSizes[0]))\n",
    "    print(model.layers[-1].output_shape)\n",
    "    # LSTM layer with a many-to-one implementation\n",
    "    # Note: size of the output = (outputSizeLastConv, outputLSTM)\n",
    "    model.add(Reshape((outputSizeLastConv,3*nkerns[0]))) \n",
    "    model.add(LSTM(outputLSTM,return_sequences=False))\n",
    "    print('LSTM output size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    # Fully-connected layer\n",
    "    model.add(Dense(inputMLP, activation=activationMLP))\n",
    "    print('Dense layer size: ---------------------------')\n",
    "    print(model.layers[-1].output_shape)\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    # Softmax layer\n",
    "    if withSoftmax:\n",
    "        model.add(Dense(nbClasses, activation='softmax'))\n",
    "\n",
    "    # Print the summary of the model\n",
    "    model.summary\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "model = normConv1Lstm(\n",
    "    inputShape=(400,3,9),\n",
    "    nkerns=[nbFeaturesLayer1, nbFeaturesLayer2, nbFeaturesLayer3],\n",
    "    filterSizes=[filter1Size, filter2Size, filter3Size],\n",
    "    poolSizes=[poolingLayer1Factor, poolingLayer2Factor, poolingLayer3Factor],\n",
    "    activationConv='relu',\n",
    "    timeWindow=400,\n",
    "    nbSensors=9,\n",
    "    outputLSTM=outputLSTM,\n",
    "    inputMLP=inputMLP,\n",
    "    activationMLP=activationMLP,\n",
    "    nbClasses=55,\n",
    "    withSoftmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:09:39.071769: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 110592000 exceeds 10% of free system memory.\n",
      "2023-05-09 17:09:39.666727: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-09 17:09:39.670801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-09 17:09:39.673760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-09 17:09:40.959786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-09 17:09:40.962279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-09 17:09:40.964487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-09 17:09:42.174515: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93600000 exceeds 10% of free system memory.\n",
      "2023-05-09 17:09:42.337638: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46800000 exceeds 10% of free system memory.\n",
      "2023-05-09 17:09:42.493155: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46800000 exceeds 10% of free system memory.\n",
      "2023-05-09 17:09:43.492159: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 3.9366 - accuracy: 0.0492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:09:51.581736: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-09 17:09:51.583943: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-09 17:09:51.585710: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 13s 1s/step - loss: 3.9366 - accuracy: 0.0492 - val_loss: 3.8028 - val_accuracy: 0.0750\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 9s 1s/step - loss: 3.6467 - accuracy: 0.1113 - val_loss: 3.5064 - val_accuracy: 0.1250\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 9s 1s/step - loss: 3.2738 - accuracy: 0.1984 - val_loss: 3.1716 - val_accuracy: 0.1797\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.9080 - accuracy: 0.2414 - val_loss: 2.9871 - val_accuracy: 0.2078\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.6241 - accuracy: 0.3164 - val_loss: 2.8104 - val_accuracy: 0.2625\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.5087 - accuracy: 0.3309 - val_loss: 2.8394 - val_accuracy: 0.2281\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.4931 - accuracy: 0.3215 - val_loss: 2.8469 - val_accuracy: 0.2438\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.4042 - accuracy: 0.3410 - val_loss: 2.6357 - val_accuracy: 0.2781\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.2153 - accuracy: 0.3840 - val_loss: 2.5698 - val_accuracy: 0.2969\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.0946 - accuracy: 0.4152 - val_loss: 2.4618 - val_accuracy: 0.2969\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.0221 - accuracy: 0.4402 - val_loss: 2.4579 - val_accuracy: 0.3141\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.9095 - accuracy: 0.4738 - val_loss: 2.3536 - val_accuracy: 0.3234\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7918 - accuracy: 0.5027 - val_loss: 2.3926 - val_accuracy: 0.3391\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7445 - accuracy: 0.5102 - val_loss: 2.2566 - val_accuracy: 0.3828\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7186 - accuracy: 0.5207 - val_loss: 2.2818 - val_accuracy: 0.3531\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.6413 - accuracy: 0.5512 - val_loss: 2.2042 - val_accuracy: 0.3891\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.6109 - accuracy: 0.5484 - val_loss: 2.2124 - val_accuracy: 0.4031\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.6500 - accuracy: 0.5234 - val_loss: 2.1191 - val_accuracy: 0.4016\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.5951 - accuracy: 0.5309 - val_loss: 2.0852 - val_accuracy: 0.4375\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.4473 - accuracy: 0.5875 - val_loss: 1.9312 - val_accuracy: 0.4531\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.3454 - accuracy: 0.6227 - val_loss: 1.9260 - val_accuracy: 0.4688\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.2773 - accuracy: 0.6445 - val_loss: 1.9324 - val_accuracy: 0.4578\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.3206 - accuracy: 0.6262 - val_loss: 1.8850 - val_accuracy: 0.4703\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.2814 - accuracy: 0.6391 - val_loss: 1.8671 - val_accuracy: 0.4734\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.1816 - accuracy: 0.6656 - val_loss: 1.7582 - val_accuracy: 0.5094\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.1422 - accuracy: 0.6719 - val_loss: 1.7402 - val_accuracy: 0.5203\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.0964 - accuracy: 0.6902 - val_loss: 1.6826 - val_accuracy: 0.5344\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.0658 - accuracy: 0.6973 - val_loss: 1.7364 - val_accuracy: 0.5234\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.0444 - accuracy: 0.6984 - val_loss: 1.6125 - val_accuracy: 0.5625\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.9556 - accuracy: 0.7359 - val_loss: 1.5922 - val_accuracy: 0.5688\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=numberOfEpochs,\n",
    "    batch_size=batchSize\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
